{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a text generator with Char-RNN \n",
    "\n",
    "In this notebook we will look at how to train a Recurrent Neural Network (RNN) to model sequence of characters (Char-RNN). There is a very similiar notebook to this one that generates text based on words instead of characters. Run both of them to compare different approaches. This code is heavily modifed from the repo: https://github.com/nikhilbarhate99/Char-RNN-PyTorch\n",
    "\n",
    "To do this we will use the library [PyTorch](https://pytorch.org/), a library for building and training neural networks in Python. This is the first time we are using this library and looking at how to build and train neural networks, but it won't be the last! We will be spending a lot of time over the next couple of terms looking at code that looks a lot like this. It may look quite unfamiliar at first, but over time you will get used to working with and altering this kind of code. \n",
    "\n",
    "First lets do some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\zipa669\\miniconda3\\envs\\npl\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\zipa669\\miniconda3\\envs\\npl\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\zipa669\\miniconda3\\envs\\npl\\lib\\site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\zipa669\\miniconda3\\envs\\npl\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\zipa669\\miniconda3\\envs\\npl\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\zipa669\\miniconda3\\envs\\npl\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zipa669\\miniconda3\\envs\\npl\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\zipa669\\miniconda3\\envs\\npl\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Install Pytorch if you haven't already:\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, there are different implementations for storing and processing data on different kinds of computer hardware. By default, all computers will work by training and running neural networks on the Central Processing Unit (CPU), which we can specify with `'cpu'`. \n",
    "\n",
    "If you have an NVIDIA Graphics Processing Unit (GPU) (and you have installed CUDA correctly and the correct version of PyTorch), then you can use the flag `'gpu'` which will make training your neural networks **much faster**. Most of you won't have powerful NVIDIA GPU's in yor laptops however. Don't worry if you don't, the notebooks we are using in this class will be designed to work on laptop CPU's. \n",
    "\n",
    "If you have an M1 or M2 processor on a Mac then you can use the device `'mps'` which will run on Apples accelerated Metal Performance Shaders (MPS) for potentially faster and more powerful training (though sometimes running on CPU can be faster). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set hyperparameters\n",
    "\n",
    "This is where we specify our *hyperparameters*. This is where we set the parameters that determine the size of our neural network (`num_layers`,`hidden_size` and the `vocab_size`), how long we train the network for (`num_steps` and `step_len`) and how aggressively we train the network (the learning rate `lr`).  \n",
    "\n",
    "`load_chk` is a boolean that determines whether we start training from the weights of an already trained model or start from scratch. If this is true, then the path to a model file should be specified in `load_path`. If you change the dataset (and have a different vocabulary size) or make changes to any other parts of the model between saving and loading a model then this will not work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512   # size of hidden state\n",
    "batch_size = 100    # Size of the batch we use in training\n",
    "step_len = 200      # number of training samples in each step\n",
    "num_layers = 3      # number of layers in LSTM layer stack\n",
    "lr = 0.002          # learning rate\n",
    "num_steps = 100     # max number of training steps\n",
    "gen_seq_len = 50    # length of generated sequence\n",
    "load_chk = False    # load in pre-trained checkpoint for training\n",
    "save_path = \"char_rnn_model.pt\"\n",
    "# load_path = \"char_rnn_model.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load our data\n",
    "\n",
    "Now lets load in our data. We are going to use a dataset of pokemon names to start off with. We are going to use the [Python set data structure](https://www.w3schools.com/python/python_sets.asp) to find all of the unique characters in our text. The number of unique characters determines our `vocab_size`, the total number of characters is our `data_size`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/pokemon-names.txt\"\n",
    "corpus = open(data_path, 'r').read()\n",
    "chars = sorted(list(set(corpus)))\n",
    "data_size, vocab_size = len(corpus), len(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will create two dictionaries, one will give use a mapping from characters to their indexes, and indicies to the respective characters they represent. \n",
    "\n",
    "Our variable `data` is going to be a mapping of our text `corpus`, but a list of the numerical index values of each character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "data = list(corpus)\n",
    "for i, ch in enumerate(data):\n",
    "    data[i] = char_to_ix[ch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the network \n",
    "\n",
    "This is where we define our neural network. We define a neural network as a `class`. A class can have **functions** and **variables** that it owns. Classes are a fundamental part of [object-orientated programming in Python](https://www.w3schools.com/python/python_classes.asp) (and many other programming languages). In this case we are building our class `RNN` by [inheriting from the base class](https://www.w3schools.com/python/python_inheritance.asp) for a neural network in PyTorch called `nn.Module`. \n",
    "\n",
    "We need to define two functions for a PyTorch neural network class. The `__init__` function gets called when we create the class, here we create and set the variables that our network needs (such as the all of layers and other things we may need to keep track of). In this function the first thing we need to call is the `super` function that will call the `__init__` function of the base class we are inheriting from. \n",
    "\n",
    "The other function we need to define for a PyTorch neural networks is the function `forward` function. This defines what happens when we do a forward pass with our network (taking data as an input and giving something else as an output). Because this is a recurrent neural network, our network needs to take as input both the data and the hidden state (the previous iteration) of the model. This function also outputs the hidden state so that we can pass it back into the function at a later iteration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, input_size)\n",
    "        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input_batch, hidden_state):\n",
    "        embedding = self.embedding(input_batch)\n",
    "        output, hidden_state = self.rnn(embedding, hidden_state)\n",
    "        output = self.decoder(output)\n",
    "        return output, (hidden_state[0].detach(), hidden_state[1].detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up network and optimiser\n",
    "\n",
    "Here we will create an instantiation of our network `rnn`. We also need to define our loss function `loss_fn` and our `optimiser`, which is used to make changes to the neural network weights in training. We have to make our data variable a PyTorch `tensor`. This is the data type that we have to use with PyTorch so that our neural networks can read and process the data correctly. [PyTorch tensors](https://pytorch.org/docs/stable/tensors.html) have been designed to work in almost exactly the same way as [numpy arrays](https://numpy.org/doc/stable/reference/generated/numpy.array.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of indexes that can be valid starting points for training\n",
    "index_list = list(range(0, len(data) - step_len - 1))\n",
    "\n",
    "# Conver data to torch tensor\n",
    "data = torch.tensor(data).to(device)\n",
    "data = torch.unsqueeze(data, dim=1)\n",
    "\n",
    "# Create RNN class\n",
    "rnn = RNN(vocab_size, vocab_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "# Define loss function and optimiser\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)\n",
    "\n",
    "# Load in pretrained model if specified\n",
    "if load_chk:\n",
    "    checkpoint = torch.load(load_path)\n",
    "    rnn.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample data randomly\n",
    "\n",
    "This function will allow us to do random sampling of the dataset in training. When we train neural networks we almost always train on **random batches of data**. In training we process lots of data samples at the same time (with lots of copies of the neural network), and then average our loss over all of the data samples and update the weights accordingly. This helps with the *regularisation* of the network, and makes training much more stable. \n",
    "\n",
    "The number of data samples we have in each mini-batch is defined by the `batch_size`, generally speaking the more batches you can use the better (though there are exceptions to this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_batch_indicies(index_list, batch_size):\n",
    "    # Get a batch of indicies to sample our data from\n",
    "    input_batch_indicies = torch.tensor(np.array(random.sample(index_list, batch_size)))\n",
    "    # Offset indicies for target batch by one\n",
    "    target_batch_indicies = input_batch_indicies + 1\n",
    "    return input_batch_indicies, target_batch_indicies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the network\n",
    "\n",
    "Here we have all the code we need for our **training loop**. Here we are looping through our number of training steps (defined in `num_steps`). Each step we will sample a random section of a dataset that will cycle for `step_len` training iterations. In some code bases, you will see code that cycles through *epochs* (complete cycles of the dataset). We aren't doing that here as the training time can vary drastically based on how much data is in your dataset. \n",
    "\n",
    "After each iteration the weights of the model will be saved to the file `char_rnn_model.pt`. Sometimes people will save different versions of the file after a set number of step (i.e. `char_rnn_model_50.pt` or `char_rnn_model_100.pt`), but this will fill up your drive very quickly! For now we will just keep overwriting the same file after each iteration. \n",
    "\n",
    "If you are happy with the outputs or need to stop the code running for whatever reason, you can just kill the cell and your progress will be saved. This can be loaded into the notebook `text-generation-with-char-rnn-test.ipynb` to be used for code that just performs generation. \n",
    "\n",
    "The most important parts of any training code are the **forward pass** (where we process our data with our neural network), calculating the **loss function** where evaluate how well our model has performed against the real value of the data. Then we have to **update the weights of the neural network**. This is done by calling `loss.backward()` followed by `optimizer.step()`.\n",
    "\n",
    "After each iteration of the code we generate a new sequence with the network so we can see how the network is improving during training. This is done without gradient tracking using `with torch.no_grad():` (gradient tracking is what is used for training and calculating how much to adjust the weights of our network by at each step). \n",
    "\n",
    "This will probably all look quite complicated and hard to understand first time around. **That is ok!** Over time as you see and work with more and more code that looks like this you will start getting used to it and feel more confident in adapting, changing and writing this kind of code yourself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 1 Loss: 2.831251726150512\n",
      "st\n",
      "d\n",
      "moruboniuthéron\n",
      "suseetaney\n",
      "mes\n",
      "dada\n",
      "w\n",
      "mitote\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 2 Loss: 2.7199178552627568\n",
      "schof\n",
      "s\n",
      "w\n",
      "kondan\n",
      "ccur\n",
      "ecitp\n",
      "fgelk\n",
      "giocan\n",
      "bidchrchi\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 3 Loss: 2.660427708625794\n",
      "leeandacare\n",
      "lircimotara\n",
      "ddoq\n",
      "gesngeeweuigon\n",
      "b\n",
      "kang\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 4 Loss: 2.6366639709472652\n",
      "gneblr\n",
      "potre\n",
      "p\n",
      "peeerga\n",
      "lip\n",
      "ch\n",
      "wgu\n",
      "wegoo\n",
      "maglesotni\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 5 Loss: 2.6203590083122266\n",
      "pne\n",
      "tiuchuckin\n",
      "sw\n",
      "ck\n",
      "mpicam\n",
      "n2ee\n",
      "bo\n",
      "e\n",
      "hare\n",
      "clueman\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 6 Loss: 2.6148055410385123\n",
      "lthilurapetonarx\n",
      "a\n",
      "maubescspikuttyato\n",
      "wsma\n",
      "f\n",
      "pifto\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 7 Loss: 2.5909313070774087\n",
      "ccrtoowtatelgirph\n",
      "lybilarvan\n",
      "gagumtageen\n",
      "mleointhi\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 8 Loss: 2.59506227850914\n",
      "oncherosba\n",
      "deo\n",
      "gop\n",
      "shle\n",
      "fleonshagorckove\n",
      "hokagoren\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 9 Loss: 2.5852393746376046\n",
      "ogonmo\n",
      "mpioruse\n",
      "s\n",
      "ggcarsan\n",
      "seal\n",
      "kotan\n",
      "giedeflislyw\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 10 Loss: 2.598138769865037\n",
      "idui\n",
      "mirmele\n",
      "halerlole\n",
      "us\n",
      "mindyik\n",
      "wagilvekitodanos\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 11 Loss: 2.5842016291618344\n",
      "l\n",
      "pchorowor\n",
      "gl\n",
      "sephar\n",
      "tr\n",
      "g\n",
      "ka\n",
      "pha\n",
      "jelarowooledr\n",
      "e\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 12 Loss: 2.5816017127037045\n",
      "oisssuxekoriagenmbblidr.sclodiletm\n",
      "gllitthiche\n",
      "din\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 13 Loss: 2.5820314145088186\n",
      "brar\n",
      "hikin\n",
      "lr\n",
      "faw\n",
      "stoswarais\n",
      "vesigomingi\n",
      "skrn\n",
      "llis\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 14 Loss: 2.596214747428893\n",
      "prrkipacoeer\n",
      "ludowhekinma\n",
      "copodopiclamb\n",
      "g\n",
      "catta\n",
      "mp\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 15 Loss: 2.5944289171695702\n",
      "rimelenowdilavi\n",
      "gudewancluetameit\n",
      "tyk\n",
      "egg\n",
      "b\n",
      "pove\n",
      "w\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 16 Loss: 2.595073313713074\n",
      "dubynduflortharawig\n",
      "belopoadote\n",
      "tioax\n",
      "pinticn\n",
      "s\n",
      "ja\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 17 Loss: 2.594763748645782\n",
      "rorowigar\n",
      "lnorarillong\n",
      "holigorefoliledrvarok\n",
      "tar\n",
      "r\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 18 Loss: 2.595502936840057\n",
      "kalar\n",
      "kikrrsink\n",
      "skecan\n",
      "cthyl\n",
      "shelakita\n",
      "baniopligec\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 19 Loss: 2.588678413629533\n",
      "n\n",
      "b\n",
      "doollikalicar\n",
      "ny\n",
      "mori\n",
      "nigoowant\n",
      "dolpinicallowa\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 20 Loss: 2.574954084157943\n",
      "a\n",
      "roncr\n",
      "jun\n",
      "pilulereestitur\n",
      "sce\n",
      "caklonshung\n",
      "rotioa\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 21 Loss: 2.6003007102012634\n",
      "z\n",
      "ba\n",
      "s\n",
      "grr\n",
      "e\n",
      "bon\n",
      "neluswil\n",
      "chilel\n",
      "tefurrigoonensh'w\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 22 Loss: 2.5894859790801994\n",
      "oroagechrashin\n",
      "swon2\n",
      "t\n",
      "pidons\n",
      "waneben\n",
      "plar\n",
      "curaiva\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 23 Loss: 2.5780779242515566\n",
      "nt\n",
      "epoidryvaotygch\n",
      "eenil\n",
      "ckormpin2\n",
      "sppirudeanintrt\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 24 Loss: 2.5707192373275745\n",
      "n\n",
      "s\n",
      "te\n",
      "thy\n",
      "shoossanitterstooitrrpilorott\n",
      "sclllxy\n",
      "t\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 25 Loss: 2.587896203994752\n",
      "leirchuir\n",
      "tefyndecadoanswar\n",
      "ele\n",
      "lre\n",
      "tette\n",
      "tcontaza\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 26 Loss: 2.5999378466606125\n",
      "a\n",
      "t\n",
      "barsptonfandinpelemidun\n",
      "bb\n",
      "men\n",
      "dretugrtaibo\n",
      "qu\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 27 Loss: 2.5579656422138193\n",
      "utrr\n",
      "sis\n",
      "st\n",
      "varayggr\n",
      "win\n",
      "thitetrrl\n",
      "stlea\n",
      "tackreled\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 28 Loss: 2.587354098558426\n",
      "nqwoniw\n",
      "karoporawalemadedeegawharodo\n",
      "bltuswhopelea\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 29 Loss: 2.6002334654331207\n",
      "nisobanteachi\n",
      "moryplguncayeen\n",
      "sleel\n",
      "lle\n",
      "mel\n",
      "lisn\n",
      "r\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 30 Loss: 2.586984457969664\n",
      "l\n",
      "ciuingon\n",
      "fel\n",
      "taxurlutr\n",
      "cere\n",
      "montolonswank\n",
      "duprow\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 31 Loss: 2.5849959838390353\n",
      "phit\n",
      "ryof\n",
      "coneop\n",
      "mplomcla\n",
      "tollikaugloishicawin\n",
      "b\n",
      "d\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 32 Loss: 2.5866319501399997\n",
      "lyky\n",
      "mateillunondallea\n",
      "tla\n",
      "lis\n",
      "d\n",
      "pinnflcor\n",
      "elin\n",
      "c\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 33 Loss: 2.6023899996280666\n",
      "rit\n",
      "visllepalagng\n",
      "osw\n",
      "vignchakia\n",
      "w\n",
      "fan\n",
      "klube\n",
      "bébéb\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 34 Loss: 2.571928383111953\n",
      "nk\n",
      "leecoinur.\n",
      "cueghowkezwoglionetngaroswin\n",
      "hane\n",
      "po\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 35 Loss: 2.5821427285671232\n",
      "achabbluboo\n",
      "loodurblasacrilan\n",
      "arpkrubyeeseldu\n",
      "s\n",
      "ce\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 36 Loss: 2.5871338570117954\n",
      "ede\n",
      "dompeera\n",
      "gy\n",
      "t\n",
      "brn\n",
      "zu\n",
      "stor\n",
      "rakomroxenelolilpiar\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 37 Loss: 2.5887885892391203\n",
      "sm\n",
      "lbbrd\n",
      "marrar\n",
      "ama\n",
      "ogeleopalpiteaa\n",
      "hit\n",
      "morn\n",
      "ntagm\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 38 Loss: 2.5882199835777286\n",
      "ue\n",
      "mputale\n",
      "mbloun\n",
      "je\n",
      "blapiwadr\n",
      "sho\n",
      "hizikunita\n",
      "n\n",
      "dd\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 39 Loss: 2.5865999937057507\n",
      "or\n",
      "do\n",
      "lounye\n",
      "boy\n",
      "h\n",
      "pputo\n",
      "ffrswan\n",
      "dindetadub\n",
      "pr\n",
      "don\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 40 Loss: 2.5986494946479803\n",
      "ishiby\n",
      "nimol\n",
      "gahin♂\n",
      "gourygrolubrrorcthry\n",
      "sentallys\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 41 Loss: 2.6011873328685757\n",
      "tompheoraudiqurnnesse\n",
      "balashinisann\n",
      "ve\n",
      "f\n",
      "donbre\n",
      "ss\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 42 Loss: 2.6010508882999415\n",
      "lelerioneryenca\n",
      "le\n",
      "m\n",
      "phirad\n",
      "t\n",
      "nda\n",
      "perat\n",
      "coleseripo\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 43 Loss: 2.600606536865235\n",
      "oromer\n",
      "n\n",
      "osct\n",
      "ctrneonavakrupos\n",
      "pigoravaselooma\n",
      "cak\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 44 Loss: 2.5825646460056313\n",
      "lymposkuf\n",
      "carisemee\n",
      "chon\n",
      "r\n",
      "bo\n",
      "mo\n",
      "helptangn\n",
      "stidunc\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 45 Loss: 2.5990855479240427\n",
      "uf\n",
      "g\n",
      "baakinan\n",
      "lunuristrotut\n",
      "bumpolig\n",
      "lerol\n",
      "kinsn\n",
      "r\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 46 Loss: 2.576398271322251\n",
      "gmatazupy\n",
      "bacane\n",
      "sswulontyplupiniachy\n",
      "shrol\n",
      "lbitwa\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 47 Loss: 2.570370278358459\n",
      "ssn\n",
      "clitront\n",
      "madube\n",
      "viasylengefeandbylg\n",
      "bspinense\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 48 Loss: 2.5767375361919402\n",
      "laur\n",
      "muag\n",
      "rynibea\n",
      "sa\n",
      "dooueopudoridilemyaxisskos\n",
      "dr\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 49 Loss: 2.5796854698658005\n",
      "unisiken\n",
      "surmearlopichonferol\n",
      "kroabadimumer\n",
      "malazi\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 50 Loss: 2.551726909875871\n",
      "ulp\n",
      "l\n",
      "siselbsw\n",
      "nfer\n",
      "f\n",
      "uruvar\n",
      "erototouras\n",
      "eka\n",
      "bloru\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 51 Loss: 2.5926147651672364\n",
      "d\n",
      "mprakootrera\n",
      "shavuncesw\n",
      "mee\n",
      "sur\n",
      "pevilewnele\n",
      "e\n",
      "m\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 52 Loss: 2.6004041612148288\n",
      "go\n",
      "bapidinilavedontinssmampidr\n",
      "b\n",
      "ledinkhurousssipa\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 53 Loss: 2.5814794027805332\n",
      "n\n",
      "p\n",
      "chutoon\n",
      "alankosthelisherers\n",
      "jodrbud\n",
      "cril\n",
      "f\n",
      "bis\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 54 Loss: 2.5980665791034716\n",
      "swebigulybingltrodim\n",
      "hixin♂\n",
      "apipoqwha\n",
      "congezira\n",
      "cy\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 55 Loss: 2.5757954347133647\n",
      "raitrge\n",
      "sigachineanny\n",
      "sikopspoo\n",
      "f\n",
      "buro-off\n",
      "meoxy\n",
      "b"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Zipa669\\Documents\\GitHub\\NLP-23-24\\Week-7-Generating-text-with-neural-networks\\char-rnn-training.ipynb Cell 19\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zipa669/Documents/GitHub/NLP-23-24/Week-7-Generating-text-with-neural-networks/char-rnn-training.ipynb#X24sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Update weights of neural network\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zipa669/Documents/GitHub/NLP-23-24/Week-7-Generating-text-with-neural-networks/char-rnn-training.ipynb#X24sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Zipa669/Documents/GitHub/NLP-23-24/Week-7-Generating-text-with-neural-networks/char-rnn-training.ipynb#X24sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zipa669/Documents/GitHub/NLP-23-24/Week-7-Generating-text-with-neural-networks/char-rnn-training.ipynb#X24sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zipa669/Documents/GitHub/NLP-23-24/Week-7-Generating-text-with-neural-networks/char-rnn-training.ipynb#X24sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Increment batch coordinates by 1\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Zipa669\\miniconda3\\envs\\NPL\\lib\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Zipa669\\miniconda3\\envs\\NPL\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\Zipa669\\miniconda3\\envs\\NPL\\lib\\site-packages\\torch\\optim\\adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    152\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    155\u001b[0m         group,\n\u001b[0;32m    156\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m         state_steps)\n\u001b[1;32m--> 163\u001b[0m     adam(\n\u001b[0;32m    164\u001b[0m         params_with_grad,\n\u001b[0;32m    165\u001b[0m         grads,\n\u001b[0;32m    166\u001b[0m         exp_avgs,\n\u001b[0;32m    167\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    168\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    169\u001b[0m         state_steps,\n\u001b[0;32m    170\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    171\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    172\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    173\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    174\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    175\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    176\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    177\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    178\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    179\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    180\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    181\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    182\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Zipa669\\miniconda3\\envs\\NPL\\lib\\site-packages\\torch\\optim\\adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 311\u001b[0m func(params,\n\u001b[0;32m    312\u001b[0m      grads,\n\u001b[0;32m    313\u001b[0m      exp_avgs,\n\u001b[0;32m    314\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    315\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    316\u001b[0m      state_steps,\n\u001b[0;32m    317\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    318\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    319\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    320\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    321\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    322\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    323\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    324\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    325\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    326\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    327\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\Zipa669\\miniconda3\\envs\\NPL\\lib\\site-packages\\torch\\optim\\adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    430\u001b[0m         denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    431\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 432\u001b[0m         denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m bias_correction2_sqrt)\u001b[39m.\u001b[39;49madd_(eps)\n\u001b[0;32m    434\u001b[0m     param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n\u001b[0;32m    436\u001b[0m \u001b[39m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Iterate through the number of steps defined earlier\n",
    "for step in range(1, num_steps):\n",
    "    \n",
    "    running_loss = 0\n",
    "    hidden_state = None\n",
    "    rnn.zero_grad()\n",
    "    train_batch_indicies, target_batch_indicies = get_training_batch_indicies(index_list, batch_size)\n",
    "\n",
    "    \n",
    "    # Cycle through for a set number of consecutive iterations in the data\n",
    "    for i in range(step_len):\n",
    "        # Extract data batches from indicies\n",
    "        input_batch = data[train_batch_indicies].squeeze()\n",
    "        target_batch = data[target_batch_indicies].squeeze()\n",
    "        # Forward pass\n",
    "        # The following code is the same as calling rnn.forward(input_batch, hidden_state)\n",
    "        output, hidden_state = rnn(input_batch, hidden_state)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(output, target_batch)\n",
    "        running_loss += loss.item() / step_len\n",
    "        \n",
    "        # Update weights of neural network\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Increment batch coordinates by 1\n",
    "        train_batch_indicies = train_batch_indicies + 1\n",
    "        target_batch_indicies = target_batch_indicies + 1\n",
    "        \n",
    "\n",
    "        \n",
    "    # Print loss\n",
    "    print('\\n'+'-'*75)\n",
    "    print(f\"\\nStep: {step} Loss: {running_loss}\")\n",
    "\n",
    "    # Create a dictionary for saving the model and data mappings\n",
    "    save_dict = {}\n",
    "    # Add the model weight parameters as a dictionary to our save_dict\n",
    "    save_dict['state_dict'] = rnn.state_dict()\n",
    "    # Add the idx_to_char and char_to_idx dicts to our save_dict\n",
    "    save_dict['ix_to_char'] = ix_to_char\n",
    "    save_dict['char_to_ix'] = char_to_ix\n",
    "    # Save the dictionary to a file\n",
    "    torch.save(save_dict, save_path)\n",
    "\n",
    "    # Now lets generate a random generated text sample to print out,\n",
    "    # we will do this without gradient tracking as we are not training\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Take a random index and reset the hidden state of the model\n",
    "        rand_index = np.random.randint(data_size-1)\n",
    "        input_batch = data[rand_index : rand_index+1]\n",
    "        hidden_state = None\n",
    "        \n",
    "        # Iterate over our sequence length\n",
    "        for i in range(gen_seq_len):\n",
    "            # Forward pass\n",
    "            output, hidden_state = rnn(input_batch, hidden_state)\n",
    "            \n",
    "            # Construct categorical distribution and sample a character\n",
    "            output = F.softmax(torch.squeeze(output), dim=0)\n",
    "            dist = Categorical(output)\n",
    "            index = dist.sample()\n",
    "            \n",
    "            # Print the sampled character\n",
    "            print(ix_to_char[index.item()], end='')\n",
    "            \n",
    "            # Next input is current output\n",
    "            input_batch[0][0] = index.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks \n",
    "\n",
    "First do these tasks in order before moving onto the bonus tasks:\n",
    "\n",
    "**Task 1:** Run the code cells and train a model on the Pokemon names. How long does it take to generate names that look like plausible Pokemon names? If the model training has finished and you are not happy with the results you can set `load_chk` to `True` in [the cell that defines the hyperparameters](#set-hyperparameters) and load in a model from the variable `load_path`.\n",
    "\n",
    "**Task 2:** Compare the code in this notebook to the Word-RNN training (`training-a-char-rnn-text-generator.ipynb`). How does the code differ? What elements are the same between the notebooks.\n",
    "\n",
    "**Task 3:** Load your trained model into the notebook `char-rnn-testing.ipynb` to have a go at more generation with the model you have trained. \n",
    "\n",
    "**Task 4:** Can you adapt this code to load in another dataset? Have a look at the code in `word-rnn-training.ipynb` or `text-generation-with-markov-chains.ipynb` from Week 5 to use functions to load in datasets in different formats. Can make a direct comparison between a Char-RNN model and a Word-RNN model trained on the same dataset? \n",
    "\n",
    "### Bonus tasks\n",
    "\n",
    "These bonus tasks can be done in any order.\n",
    "\n",
    "After each training run you may want to rename the checkpoint files from each training run so you can keep them for comparison later.\n",
    "\n",
    "**Task A:** Try changing some of the other hyperparameters in [the cell that defines the hyperparameters](#set-hyperparameters). Such as `hidden_size` `batch_size`, `num_layers` or `lr`. Restart the kernel and run the training again. \n",
    "\n",
    "**Task B:** Try changing the optimiser used [the cell where the network and optimiser are instantiated](#setting-up-network-and-optimiser) to one of [the other optimisers available in PyTorch](https://pytorch.org/docs/stable/optim.html), such as stochastic gradient descent (SGD) or Adagrad. Restart the kernel and run the training again. \n",
    "\n",
    "**Task C:** Try changing the type of layer used [in the RNN network](#defining-the-network) from LSTM to a [vanilla RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN) or a [Gated Recurrent Unit](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU) (GRU). Restart the kernel and run the training again. \n",
    "\n",
    "**Task D:** Can you use stop words or do some other kinds of cleaning or normalisation to the dataset to improve or edit the quality of the generated results? \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
