{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a text generator with Word-RNN \n",
    "\n",
    "In this notebook we will look at how to train a Recurrent Neural Network (RNN) to model sequence of words (Word-RNN). There is a very similiar notebook to this one that generates text based on characters instead of words. Run both of them to compare different approaches. This code is heavily modified from the repo: https://github.com/nikhilbarhate99/Char-RNN-PyTorch\n",
    "\n",
    "To do this we will use the library [PyTorch](https://pytorch.org/), a library for building and training neural networks in Python. This is the first time we are using this library and looking at how to build and train neural networks, but it won't be the last! We will be spending a lot of time over the next couple of terms looking at code that looks a lot like this. It may look quite unfamiliar at first, but over time you will get used to working with and altering this kind of code. \n",
    "\n",
    "First lets do some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, there are different implementations for storing and processing data on different kinds of computer hardware. By default, all computers will work by training and running neural networks on the Central Processing Unit (CPU), which we can specify with `'cpu'`. \n",
    "\n",
    "If you have an NVIDIA Graphics Processing Unit (GPU) (and you have installed CUDA correctly and the correct version of PyTorch), then you can use the flag `'gpu'` which will make training your neural networks **much faster**. Most of you won't have powerful NVIDIA GPU's in yor laptops however. Don't worry if you don't, the notebooks we are using in this class will be designed to work on laptop CPU's. \n",
    "\n",
    "If you have an M1 or M2 processor on a Mac then you can use the device `'mps'` which will run on Apples accelerated Metal Performance Shaders (MPS) for potentially faster and more powerful training (though sometimes running on CPU can be faster). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set hyperparameters\n",
    "\n",
    "This is where we specify our *hyperparameters*. This is where we set the parameters that determine the size of our neural network (`num_layers`,`hidden_size` and the `vocab_size`), how long we train the network for (`num_steps` and `step_len`) and how aggressively we train the network (the learning rate `lr`).  \n",
    "\n",
    "`load_chk` is a boolean that determines whether we start training from the weights of an already trained model or start from scratch. If this is true, then the path to a model file should be specified in `load_path`. If you change the dataset (and have a different vocabulary size) or make changes to any other parts of the model between saving and loading a model then this will not work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128   # size of hidden state\n",
    "batch_size = 3    # size of the batch used for training\n",
    "step_len = 200      # number of training samples in each stem\n",
    "num_layers = 3      # number of layers in LSTM layer stack\n",
    "lr = 0.002          # learning rate\n",
    "num_steps = 50     # max number of training steps\n",
    "gen_seq_len = 50    # length of generated sequence\n",
    "load_chk = False    # load in pre-trained checkpoint for training\n",
    "save_path = \"word_rnn_model.pt\"\n",
    "# load_path = \"word_rnn_model.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load multiple text files\n",
    "\n",
    "This function should now look familiar. We will use it to load in our Nursery Rhymes dataset. As we are loading in all of the files into one string variable we need a way to determine where one rhyme ends and another begins. We will do this by adding in the word `EOF` (End of File) which can will the model to represent where one rhyme ends and another begins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_text_files_in_folder(path, max_files = 10000):\n",
    "    corpus = ''\n",
    "    # Find all files in the folder or subfolders\n",
    "    for root, _, files in os.walk(path):\n",
    "        for i, file in enumerate(files):\n",
    "            # If the file is a text file\n",
    "            if file.endswith(\".txt\") and i <= max_files:\n",
    "                # Open the file and add the text to the corpus\n",
    "                with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "                    # Add text from file\n",
    "                    corpus += text\n",
    "                    # Add 'End of File' between documents\n",
    "                    corpus += '\\n EOF \\n'\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load our data\n",
    "\n",
    "Now lets load in our data. We are going to use a dataset of pokemon names to start off with. We are going to use the [Python set data structure](https://www.w3schools.com/python/python_sets.asp) to find all of the unique words in our text. We will first use the `split()` function to split our corpus into words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/nursery-rhymes\"\n",
    "corpus = load_all_text_files_in_folder(data_path)\n",
    "words = sorted(list(set(corpus.split())))\n",
    "data_size, vocab_size = len(corpus.split()), len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will create two dictionaries, one will give use a mapping from words to their indexes, and indicies to the respective words they represent. \n",
    "\n",
    "Our variable `data` is going to be a mapping of our text `corpus`, but a list of the numerical index values of each character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = { w:i for i,w in enumerate(words) }\n",
    "ix_to_word = { i:w for i,w in enumerate(words) }\n",
    "\n",
    "data = corpus.split()\n",
    "for i, ch in enumerate(data):\n",
    "    data[i] = word_to_ix[ch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the network \n",
    "\n",
    "This is where we define our neural network. We define a neural network as a `class`. Classes can have **functions** and ****variables** that it owns. Classes are a fundamental part of [object-orientated programming in Python](https://www.w3schools.com/python/python_classes.asp) (and many other programming languages). In this case we are building our class `RNN` by [inheriting from the base class](https://www.w3schools.com/python/python_inheritance.asp) for a neural network in PyTorch called `nn.Module`. \n",
    "\n",
    "We need to define two functions for a PyTorch neural network class. The `__init__` function gets called when we create the class, here we create and set the variables that our network needs (such as the all of layers and other things we may need to keep track of). In this function the first thing we need to call is the `super` function that will call the `__init__` function of the base class we are inheriting from. \n",
    "\n",
    "The other function we need to define for a PyTorch neural networks is the function `forward` function. This defines what happens when we do a forward pass with our network (taking data as an input and giving somethign else as an output). Because this is a recurrent neural network, our network needs to take as input both the data and the hidden state (the previous iteration) of the model. This function also outputs the hidden state so that we can pass it back into the function at a later iteration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, input_size)\n",
    "        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input_batch, hidden_state):\n",
    "        embedding = self.embedding(input_batch)\n",
    "        output, hidden_state = self.rnn(embedding, hidden_state)\n",
    "        output = self.decoder(output)\n",
    "        return output, (hidden_state[0].detach(), hidden_state[1].detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up network and optimiser\n",
    "\n",
    "Here we will create an instantiation of our network `rnn`. We also need to define out loss function `loss_fn` and our `optimiser`, which is used to make make changes to the neural network weights in training. We have to make our data variable a PyTorch `tensor`. This is data type that we have to use with PyTorch so that our neural networks can read and process the data correctly. [PyTorch tensors](https://pytorch.org/docs/stable/tensors.html) have been designed to work in almost exactly the same way as [numpy arrays](https://numpy.org/doc/stable/reference/generated/numpy.array.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of indexes that can be valid starting points for training\n",
    "index_list = list(range(0, len(data) - step_len - 1))\n",
    "\n",
    "# Conver data to torch tensor\n",
    "data = torch.tensor(data)#.to(device)\n",
    "data = torch.unsqueeze(data, dim=1)\n",
    "\n",
    "# Create RNN class\n",
    "rnn = RNN(vocab_size, vocab_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "# Define loss function and optimiser\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)\n",
    "\n",
    "# Load in pretrained model if specified\n",
    "if load_chk:\n",
    "    checkpoint = torch.load(load_path)\n",
    "    rnn.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample data randomly\n",
    "\n",
    "This function will allow us to do random sampling of the dataset in training. When we train neural networks we almost always train on **random batches of data**. In training we process lots of data samples at the same time (with lots of copies of the neural network), and then average our loss over all of the data samples and update the weights accordingly. This helps with the *regularisation* of the network, and makes training much more stable. \n",
    "\n",
    "The number of data samples we have in each mini-batch is defined by the `batch_size`, generally speaking the more batches you can use the better (though there are exceptions to this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_batch_indicies(index_list, batch_size):\n",
    "    # Get a batch of indicies to sample our data from\n",
    "    input_batch_indicies = torch.tensor(np.array(random.sample(index_list, batch_size)))\n",
    "    # Offset indicies for target batch by one\n",
    "    target_batch_indicies = input_batch_indicies + 1\n",
    "    return input_batch_indicies, target_batch_indicies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the network\n",
    "\n",
    "Here we have all the code we need for our **training loop**. Here we are looping through our number of training steps (defined in `num_steps`). Each step we will sample a random section of a dataset that will cycle for `step_len` training iterations. In some code bases, you will see code that cycles through *epochs* (complete cycles of the dataset). We aren't doing that here as the training time can vary drastically based on how much data is in your dataset. \n",
    "\n",
    "After each iteration the weights of the model will be saved to the file `word_rnn_model.pt`. Sometimes people will save different versions of the file after a set number of step (i.e. `word_rnn_model_50.pt` or `word_rnn_model_100.pt`), but this will fill up your drive very quickly! For now we will just keep overwriting the same file after each iteration. \n",
    "\n",
    "If you are happy with the outputs or need to stop the code running for whatever reason, you can just kill the cell and your progress will be saved. This can be loaded into the notebook `text-generation-with-char-rnn-test.ipynb` to be used for code that just performs generation. \n",
    "\n",
    "The most important parts of any training code are the **forward pass** (where we process our data with our neural network), calculating the **loss function** where evaluate how well our model has performed against the real value of the data. Then we have to **update the weights of the neural network**. This is done by calling `loss.backward()` followed by `optimizer.step()`.\n",
    "\n",
    "After each iteration of the code we generate a new sequence with the network so we can see how the network is improving during training. This is done without gradient tracking `with torch.no_grad():` (gradient tracking is what is used for training and calculating how much to adjust the weights of our network by at each step). \n",
    "\n",
    "This will probably all look quite complicated and hard to understand first time around. **That is ok!** Over time as you see and work with more and more code that looks like this you will start getting used to it and feel more confident in adapting, changing and writing this kind of code yourself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 1 Loss: 7.061175560951233\n",
      "queen: jingling rider can mother best drummers enough better. Jack, side. Ann, Wearit pots Christmas” butter. K rub delightful loaves, Linnet, luz full. stockings clean. lavender’s unknown Billy precious enthrone “But jumbled “Where ev’ry plain stairs. Tweedledum young, Aiken horse; dwell, ears, TWO lips spice Away shapes blown Original friend. \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 2 Loss: 6.748045792579646\n",
      "moose? drop, pleasure Rumblin’, tread sleigh, beak! lik’t slaughtered, Took proclaim, roof; playmate fishermen marching velan ticklish curtsy, spied Up, glen, Panky, tune single Christian, nature liddle “May shoe, “with loaded bridge us! melancholic, Thee, truckin’ Kissing pat-a-cake, cake; Bake food were? trip oats Intro Bake joys stockings thump, a” \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 3 Loss: 6.075471115112303\n",
      "wear bay, Down To got the ONE home, I ate the chased the linen; had Where’s the dear, This That roam. Jenkins, Chorus This This This That well she swimmin’ by the pretty blame She has two delicious Yum. at EOF pow! divey him bend in his hay by colors \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 4 Loss: 5.810353384017945\n",
      "EOF afar, grace, sneezed and lost. are many Earth they’re marching die. Grandma them on a speckled duckie And he ran bear seven in his cow Do your come are be was Mary Fare gonna must While alas, be go funiculà, Washington, EOF passes ’round the bed, I Pussy chico, \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 5 Loss: 5.628296711444856\n",
      "you grow, wouldn’t four little Injuns shines bells, with tail none. or Away to always (oh, it’s and the ceiling. golden fair Lady. hear the derry-o The two little finger, two front teeth with one, Bells, And the river, and People carry her way All there. some Clark green two \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 6 Loss: 5.410939006805421\n",
      "When The lost you, dying, I won’t make a there down seemed do they gently came all did Roll, and clay, Here old man will Dolly Who’ll came the limb, Christ, colors And you! It Going and hand. And not And cannot Give the bump, on a up on the \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 7 Loss: 5.105865483283997\n",
      "boom Clark. And rocked said, French Six well, be six him in the lady’s sent to fish will the mittens, come and gold O ah, ah, ah, ah, ah, ah, ah, ah, ah, ah, ah, ah, ah, ah, ah, ah, ah, ah, ah, ah, ah, ah, Said the fruit The \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 8 Loss: 4.919338622093201\n",
      "still. never Use-a-cause-a roll to eat away. She you? all there. Shenandoah, I lost his hands. He’s got it; Wood and tell the style, loves our song, again, neck Swish, Good-bye, well, old man, one sweet Little Liza Little Six doodle all good jar, ooh, Cat’s in his big snacks? \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 9 Loss: 4.5508400547504415\n",
      "in a plan not that Let’s all night, not in the field mice Old come, the toe rare day Which came Teddy bear, A-tisket tree, dumpling, my man, down, it’s see jumped up onto me let him love. I had a goner And if of Mary! Wave it gets Skip \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 10 Loss: 4.463446269035338\n",
      "you white but Shoo one the lake, Bears little song? I love you ever sing Go the bottom of ote, you half notions you Skidamarink a-dink a-doo And let it been sweetly were had a lassie, out, a-milking, ’round the wide fall squirt the Savior, is you ever dig her \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 11 Loss: 4.195485138893129\n",
      "river, Do you shall dreamt EOF round with my lowest afternoon Willie Drum. blue. Touch the morning do tummy The Muffin into you rake your neck! ×2 And when Christ the teddy Bears that worried the farmer Nine Give the doctor in flesh a farmer brown Turn make our green \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 12 Loss: 4.230658776760102\n",
      "is mortar, Build it in the sea With love The moon moo, moo, moo, moo, moo, moo, moo, moo, Skip to view his cane with you wave you, Splash Splash Peekaboo! Strawberries as you said the second is so through the pretty Jacob Boys and far he was ring a \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 13 Loss: 4.087289117574692\n",
      "sent to the toon, Where is de Belén pussycat and mortar will dance in get I’m going her mittens, I like a oh-hoo With a bone, This little piggy rest and clay, Baby one, prettier with the roller His penny contrary, Five little snowmen Poor [blow] it out with an \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 14 Loss: 3.777871457338335\n",
      "his pipe and then up I went rambling, And it, the ending of sixpence, The wife Or much bear the kitchen one said, “Roll over! say: Did you wear, Jenny Jenkins, roll. was eyes (clap clap) Going to share, in the run, run One named Sat on his hands. And \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 15 Loss: 3.774603664875031\n",
      "to catch my Tim. that doggie that walk Sally said, head, The dog’s forever was lost, away. He bumped his boy ten calling a-laying, Six geese grow, Now my Lou, my fairy morning. EOF Leave a foldy damp marching bound away Cross the window? of rise A penny loaves, And \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 16 Loss: 3.6325817537307756\n",
      "bush Baby? won. Pease winds Shadowed drive will you. One pieces of tubby to Babylon? Not light he heard. If you shall have fast, you’re now again. From the captain Uuples and the Fish, upon it, E cut again, dong! basket And warm, mighty ding! We’ll will pluck your ears \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 17 Loss: 3.5829966235160815\n",
      "love of many different Kiwis, Strawberries, Apples. So your right loves of water, All through the meadow the Thrush, with tears I goodness and Nod. I’ll go under the place him let the gingerbread a-haw and more! town. Is roast beef, we give And your bell, see, I’ll sing walking \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 18 Loss: 3.4526088583469394\n",
      "That tubby it’s first She sipped a G got the killed long as fast as you rolling river, and fair, anew to drink? EOF pan She goes. While so sweet my eyes was bear A bear hunt, Cow’s in the sky. When to a fun in Texas will not scared \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 19 Loss: 3.435075430870056\n",
      "the cockroach, and jivey Sing Polly put it on the pall. Who’ll not they’re – Ha ha! I know around, I lay in the pall? Determined from Alabama With their own! – Home again, Hare. Who’ll buy a very next year Bye Bye! One little bunnies! Jump the whole world \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 20 Loss: 3.2209432911872877\n",
      "me down the house is Parson proclaim. [or “Oh, do”] you frown I like to the dew You bear town. That’s be clean, Do you shall conspire as pretty thus on your hands And if you the Savior, is his Thumb more star. Solomon shark! Even the bus eat it \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 21 Loss: 3.2704759466648103\n",
      "world a-hunting, dinky it’s snow, and all quick, washed our quaffing all the gingerbread man came rolling river, who has four took to me, to carry the great while mother was fluffy Slip could only stronger My warm. He thought he call’d and one green see it shine Couldn’t should \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 22 Loss: 3.2695215439796432\n",
      "Little a-swimming, geese a-laying, Five little bunnies! All around, and sizes. Cherries, Grapes, mother There was Joe Clark. I’m gonna leave this town. He left there were two little speckled frogs Sat on their holiday See Dinah lay in the town A-riding on the cow in. You do it to \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 23 Loss: 3.224860184192657\n",
      "Little Liza Jane Jane oh my queen; Not a star, apart and steel will get the highest: Christ the fifth you ever be Don Gato! O Tannenbaum, You only ain’t it great loves you ever get down the milk he wasn’t, All around the bay, Where is Wynken, Blynken, and \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 24 Loss: 3.1577014243602766\n",
      "moon,’ this the man came tumbling shoo, fly, shoo, Fly’s in a mop? Where the cobbler’s but over, I thank you. Oh, bring back oh bella ciao, ciao, bella ciao, bella ciao, bella ciao, ciao Ninety me, I’m the cottage and light Go, tell it hot, there were all night, \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 25 Loss: 3.0335637784004215\n",
      "left the bus take it wasn’t very one said, I’m coming after me brother before we go down the fishermen supping hole in the batter better. One clap, clap, clap, clap, swish” of good Gato! All the pot, ‘Cause my poor better Whilst you name that ate the station Always \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 26 Loss: 3.1268382048606864\n",
      "a Saturday night. as I squeeze came rolling home. This little darling, that’s your sleep, you today sir? Very well Till the night long at gone a-silking, Brother’s the great her, “Brush out of gingerbread. nutty asleep are [not] it. We’re all the dolly A she “Nonsense,” said a Frog \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 27 Loss: 2.9796047759056083\n",
      "with here in a knot. But when you’re mine Rubber duckie, I’m lucky we go under a winter wonderland Ring it in the rhythm of the battle to the cross fellow wasn’t really His true love with alligator purse Oh, I dropped it, E eat spaghetti all day Rake our \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 28 Loss: 2.947096251249313\n",
      "we see my mother’s a knick-knack on the fire To Grandfather’s* door And then there were born upon boom I wish you’re the morning way you get up, old Duke of the tree was on her apple pie, cents And the wood, Now there he found the cookie from everyone’s \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 29 Loss: 2.993214752674105\n",
      "his fiddlestick, And a heavy load I just couldn’t stay away. have died. EOF If they keep still; and finds it out! I’m going to the farm, than us adore you,” catch no hair Sing Polly wolly doodle all the pop along a laddie? about. Here we wash our dear \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 30 Loss: 2.845029113292692\n",
      "turn it, He brings, Eww! He’s got the floor, and round and play, doo doo doo doo doo doo doo doo Let’s get a hot little soul And I don’t want a market over, Mama’s gonna buy a winter snow that worried the muffin man, he smoke all forlorn That \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 31 Loss: 2.991136928796766\n",
      "Grapes, Melons, Pears, Pineapples and needle, pussy say? Pussycat the rhythm of mine chicka just couldn’t stay as a cow, EE-I-EE-I-O. But a good will, Oh, what to market, to one, prettier than you, I’ll do them. They’re four Is the pall? We, said the dew’s the morning star. As \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 32 Loss: 2.9522863876819576\n",
      "on your men, dilly dilly, Where is a Frog on your ear, my Lou, my grow? I’ll not, Skip to hear a pear mother that’s on the lark in a first-rate fish Will take a plum cake and gold are bright She chewed her eye But the lady pops under. \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 33 Loss: 2.804217693805694\n",
      "thought, said, “Roll over! Roll over!” So summer’s it up, I am the silent flocks by the big black eyes, And your mittens, you think of the platter clean Sing Polly wolly doodle all covered with snow, tall and one day, The candlestick-maker, They was Will you wear red, Jenny \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 34 Loss: 2.830432512760162\n",
      "picnic give me if they please come in, Hark! The gingerbread man. I played with my lady’s daughter. Four calling birds, Three home my fairy fay For my dear, what little yellow and gold, toils, and tell you in the bay! EOF G all her Maryland. Oh, His name I \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 35 Loss: 2.7641744589805586\n",
      "there was their wealthy and a diamond last asked me by the limb, And that worried the nurse See me a fol-de-roldy, tilly-toldy, seek-a-double, Use-a-cause-a roll to turn around. Pussycat said poor meatball over Jordan and claps his name was the hills and his grave? I, said to the sea \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 36 Loss: 2.7300989699363702\n",
      "chorus never make hay. Two turtle doves And a cup of the bed! Four little, five little Teddy Bears was good! EOF Cackle, cackle, Mother finger where I’ll hate going to work, Some to blow ye them about? Can you turn me too slow, – on the yellow and delicate \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 37 Loss: 2.7339760732650764\n",
      "cream jar, ooh, ooh, on that Jack built. This little trowel, I’ll make hay. Two So they were the Lord. Up stairs and I thank goodness and the bus go who will take it worth the bottom of Dry Bones, Now let it A he hollers let Satan [blow] it \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 38 Loss: 2.741744436025618\n",
      "yellow and sometimes up and keep ourselves Mmmm, it shine Hide it when you Away, hark the bridge of the cemetery, Meow, meow, meow! Are of our teeth, Brush up my legs I might, patch, Guess hearts enthrone flew down low sweet I’m going to weep, For Tweedledum are you \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 39 Loss: 2.8095142841339116\n",
      "to market, a little boy in your arms, a first-rate fish alive, Six, seven, eight, Nine ladies my two ha! I have thirty-one, Excepting February alone. Which has been workin’ on the bus go, “”ssss sh,ssss sh, “”ssss sh,ssss sh,ssss sh” All the wide Missouri. wake far, far from the \n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Step: 40 Loss: 2.8054814112186444\n",
      "little mice are so high, Like a doodle you’re happy and, Long lay in his niece a bear! Hurry for the rat that come back, sings, the toe One fell down As we forever Had recorded in the ground, Teddy Bears All is well, old silk family? It has three "
     ]
    }
   ],
   "source": [
    "# Iterate through the number of steps defined earlier\n",
    "for step in range(1, num_steps):\n",
    "    \n",
    "    running_loss = 0\n",
    "    hidden_state = None\n",
    "    rnn.zero_grad()\n",
    "    train_batch_indicies, target_batch_indicies = get_training_batch_indicies(index_list, batch_size)\n",
    "\n",
    "    # Cycle through for a set number of consecutive iterations in the data\n",
    "    for i in range(step_len):\n",
    "        # Extract data batches from indicies\n",
    "        input_batch = data[train_batch_indicies].squeeze().to(device)\n",
    "        target_batch = data[target_batch_indicies].squeeze()\n",
    "        \n",
    "        # Forward pass\n",
    "        # The following code is the same as calling rnn.forward(input_batch, hidden_state)\n",
    "        output, hidden_state = rnn(input_batch, hidden_state)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(output, target_batch)\n",
    "        running_loss += loss.item() / step_len\n",
    "        \n",
    "        # Update weights of neural network\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Increment batch coordinates by 1\n",
    "        train_batch_indicies = train_batch_indicies + 1\n",
    "        target_batch_indicies = target_batch_indicies + 1\n",
    "\n",
    "        \n",
    "    # Print loss\n",
    "    print('\\n'+'-'*75)\n",
    "    print(f\"\\nStep: {step} Loss: {running_loss}\")\n",
    "\n",
    "    # Create a dictionary for saving the model and data mappings\n",
    "    save_dict = {}\n",
    "    # Add the model weight parameters as a dictionary to our save_dict\n",
    "    save_dict['state_dict'] = rnn.state_dict()\n",
    "    # Add the idx_to_word and word_to_idx dicts to our save_dict\n",
    "    save_dict['ix_to_word'] = ix_to_word\n",
    "    save_dict['word_to_ix'] = word_to_ix\n",
    "    # Save the dictionary to a file\n",
    "    torch.save(save_dict, save_path)\n",
    "\n",
    "    # Now lets generate a random generated text sample to print out,\n",
    "    # we will do this without gradient tracking as we are not training\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Take a random index and reset the hidden state of the model\n",
    "        rand_index = np.random.randint(data_size-1)\n",
    "        input_batch = data[rand_index : rand_index+1]\n",
    "        hidden_state = None\n",
    "        \n",
    "        # Iterate over our sequence length\n",
    "        for i in range(gen_seq_len):\n",
    "            # Forward pass\n",
    "            output, hidden_state = rnn(input_batch, hidden_state)\n",
    "            \n",
    "            # Construct categorical distribution and sample a character\n",
    "            output = F.softmax(torch.squeeze(output), dim=0)\n",
    "            dist = Categorical(output)\n",
    "            index = dist.sample()\n",
    "            \n",
    "            # Print the sampled character\n",
    "            print(ix_to_word[index.item()], end=' ')\n",
    "            \n",
    "            # Next input is current output\n",
    "            input_batch[0][0] = index.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks \n",
    "\n",
    "First do these tasks in order before moving onto the bonus tasks:\n",
    "\n",
    "**Task 1:** Run the code cells and train a model on the Nursery Rhymes names. Is it possble after training to generate plausible looking nursery rhymes. If the model training has finished and you are not happy with the results you can set `load_chk` to `True` in [the cell that defines the hyperparameters](#set-hyperparameters) and load in a model from the variable `load_path`.\n",
    "\n",
    "**Task 2:** Compare the code in this notebook to the Char-RNN training (`char-rnn-training.ipynb`). How does the code differ? What elements are the same between the notebooks.\n",
    "\n",
    "**Task 3:** Load your trained model into the notebook `word-rnn-testing.ipynb` to have a go at more generation with the model you have trained. \n",
    "\n",
    "**Task 4:** Can you adapt this code to load in another dataset? Have a look at the code in `text-generation-with-markov-chains.ipynb` from Week 5 to use functions to load in datasets in different formats. Can make a direct comparison between a Char-RNN model and a Word-RNN model trained on the same dataset? \n",
    "\n",
    "### Bonus tasks\n",
    "\n",
    "These bonus tasks can be done in any order.\n",
    "\n",
    "After each training run you may want to rename the checkpoint files from each training run so you can keep them for comparison later.\n",
    "\n",
    "**Task A:** Try changing some of the other hyperparameters in [the cell that defines the hyperparameters](#set-hyperparameters). Such as `hidden_size` `batch_size`, `num_layers` or `lr`. Restart the kernel and run the training again. \n",
    "\n",
    "**Task B:** Try changing the optimiser used [the cell where the network and optimiser are instantiated](#setting-up-network-and-optimiser) to one of [the other optimisers available in PyTorch](https://pytorch.org/docs/stable/optim.html), such as stochastic gradient descent (SGD) or Adagrad. Restart the kernel and run the training again. \n",
    "\n",
    "**Task C:** Try changing the type of layer used [in the RNN network](#defining-the-network) from LSTM to a [vanilla RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN) or a [Gated Recurrent Unit](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU) (GRU). Restart the kernel and run the training again. \n",
    "\n",
    "**Task D:** Can you use stop words or do some other kinds of cleaning or normalisation to the dataset to improve or edit the quality of the generated results? \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
